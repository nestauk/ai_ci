{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrunchBase exploratory analysis\n",
    "\n",
    "This notebook explores CrunchBase data to analyse the distribution of AI companies by sector. \n",
    "\n",
    "It was initially created for our collaborative MSc supervision with the Bank of England but is also used in our analysis of UK creative industries\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Load data from Nesta DAPS\n",
    "2. Identify a suitable sectoral category\n",
    "3. Flag AI companies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_getters.core import get_engine\n",
    "from data_getters.inspector import get_schemas\n",
    "from random import sample\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "today_str = str(datetime.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create this type to deal with some Nones later\n",
    "NoneType = type(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection to the database\n",
    "\n",
    "my_config_here =\"../mysqldb_team.config\"\n",
    "\n",
    "con = get_engine(my_config_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read organisations\n",
    "comps_chunks = pd.read_sql_table('crunchbase_organizations', con, chunksize=1000)\n",
    "\n",
    "#Read categories\n",
    "cats_chunks = pd.read_sql_table('crunchbase_organizations_categories', con, chunksize=1000)\n",
    "\n",
    "#Concatenate the chunks into dfs\n",
    "comps, cats = [pd.concat(x).reset_index(drop=True) for x in [comps_chunks,cats_chunks]]\n",
    "#descr_short, descr_long = [comps[v].apply(lambda x: x.lower() if type(x)==str else np.nan) for v in ['short_description','long_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(comps))\n",
    "\n",
    "print(len(cats))\n",
    "\n",
    "print(len(set(cats['organization_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 600,000K organisations without categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slight data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge organisations and categories\n",
    "\n",
    "#We reset the index because you can't merge series\n",
    "cats_grouped = cats.groupby('organization_id')['category_name'].apply(lambda x: list(x)).reset_index(drop=False)\n",
    "\n",
    "#This gives us a dataframe with a new field with the list of categories for the organisation\n",
    "comps_cats = pd.merge(comps,cats_grouped,left_on='id',right_on='organization_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as expected given the length of dfs above (there seems to be a small number of organisations in the `cat` df not included in the `comps` df but we can worry about that a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to focus on company entities.\n",
    "#There are some missing values in roles, which we would also drop (Note the False in the control flow)\n",
    "\n",
    "comps_cats = comps_cats.loc[['company' in x if type(x)!=NoneType else False for x in comps_cats['roles']]]\n",
    "\n",
    "comps_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify 'AI' companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find AI companies based on text description or the AI category\n",
    "\n",
    "#These are the terms we use for now. TODO: expand these using semantic similarities\n",
    "ai_terms = ['data science','machine learning', 'deep learning','artificial intelligence','neural network', ' ai ','natural language processing','text mining']\n",
    "\n",
    "#Lowercase the text\n",
    "comps_cats['long_description'] = comps_cats['long_description'].apply(lambda x: x.lower() if type(x)!=NoneType else np.nan)\n",
    "\n",
    "#Count the number of times that a company mentions AI \n",
    "comps_cats['ai_text_n'] = [sum([term in x for term in ai_terms]) if pd.isnull(x)==False else np.nan for x in comps_cats['long_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_cats['ai_text_n'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most companies that mention AI do this once. Others mention it more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check a few companies with more than 3 AI mentions to see what they do\n",
    "for x in sample(list(comps_cats.loc[comps_cats['ai_text_n']>3]['long_description']),5):\n",
    "    print(x)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we check AI in categories. Note there might be other relevant categories in the data but we will not do this for now\n",
    "comps_cats['ai_cats'] = ['artificial intelligence' in c for c in comps_cats['category_name']]\n",
    "\n",
    "comps_cats['ai_cats'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the overlap between companies with AI categories and AI relateed text in the description?\n",
    "comp_cats_frequences = pd.crosstab(comps_cats['ai_text_n'],comps_cats['ai_cats'])\n",
    "\n",
    "#What is the distribution of companies that mention AI various times over the share of companies with AI in their category?\n",
    "comp_cats_frequences['text_share'] = 100*comp_cats_frequences[True]/comp_cats_frequences.sum(axis=1)\n",
    "\n",
    "comp_cats_frequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around a third of companies with AI in their categories don't mention AI related terms in their descriptions.\n",
    "There are quite a few companies that mention AI repeatedly but don't have an AI category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of company descriptions for companies that have AI cats but no AI Terms\n",
    "\n",
    "#Check a few companies with more than 3 AI mentions to see what they do\n",
    "for x in sample(list(comps_cats.loc[(comps_cats['ai_text_n']==0)&(comps_cats['ai_cats']==True)]['long_description']),5):\n",
    "    print(x)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The companies that have ai categories but no ai related terms in their description look quite noisy. Let's exclude them from the analysis for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flag as AI companies with at least one AI term in their description. Later we could change this threshold\n",
    "comps_cats['ai_flag'] = comps_cats['ai_text_n']>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of descriptive analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to create a year variable (founded on is currently a date)\n",
    " \n",
    "comps_cats['founded_year'] = [x.year if type(x)!=NoneType else np.nan for x in comps_cats['founded_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "(100*pd.crosstab(comps_cats['founded_year'],comps_cats['ai_flag'],normalize=1)).plot(ax=ax,title='Year share of activity')\n",
    "\n",
    "ax.set_xlim(2000,2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting: explosion of AI startup activity while startup activity in general slows-up. What else could be explaining this? China's entry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate country distribution\n",
    "country_distr = pd.crosstab(comps_cats['country'],comps_cats['ai_flag']).sort_values(True,ascending=False)\n",
    "\n",
    "country_distr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the coverage issues are apparent - relatively limited activity in Japan. And where is China?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate index of comparative advantage\n",
    "country_distr['ai_rca']= (country_distr[True]/country_distr[True].sum())/(country_distr.sum(axis=1)/country_distr.sum(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot RCAs for top 20 countries by level of activity\n",
    "(country_distr[:20]['ai_rca'].sort_values(ascending=False)-1).plot.bar(title='Relative specialisation in AI for top 20 countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results are expected (Israel, Singapore). Others (Canada), not so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That gnarly pivot gives us the number of ai companies per year and country.\n",
    "ai_country_counts = pd.pivot_table(comps_cats.groupby(['founded_year','country'])['ai_flag'].sum().reset_index(drop=False),index='country',columns='founded_year',values='ai_flag').fillna(0)\n",
    "\n",
    "#We want to focus our visualisation on the top 10 countries by overall activity\n",
    "bigger_countries = ai_country_counts.sum(axis=1).sort_values(ascending=False).index[:15]\n",
    "\n",
    "#Consider share of activity in a given year\n",
    "\n",
    "ai_country_shares = ai_country_counts.apply(lambda x: x/x.sum(),axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ai_country_shares.loc[bigger_countries].T.rolling(window=3).mean().plot(ax=ax,title='Share of year in Country',figsize=(10,5),cmap='tab20',linewidth=2)\n",
    "\n",
    "ax.set_xlim(2000,2018)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone seems to be following a similar patterns perhaps with the exception of Singapore and Switzerland, which seem to be growing faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider country sizes (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product, chain\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in a_list for x in el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "#Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in comps_cats['category_name']])\n",
    "sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "#Turn the sector combs into an edgelist\n",
    "edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "edge_list['weight']=1\n",
    "\n",
    "#Group over edge pairs to aggregate weights\n",
    "edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "edge_list_weighted.sort_values('weight',ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create network and extract communities\n",
    "net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "\n",
    "#We choose a high level of resulution (lower == more finely grained)\n",
    "comms = community.best_partition(net,resolution=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does this look like?\n",
    "comm_strings = pd.DataFrame(comms,index=['comm']).T.groupby('comm')\n",
    "\n",
    "#This is just to visualise the participation in communities\n",
    "for n,x in enumerate(comm_strings.groups.keys()):\n",
    "    print(n)\n",
    "    print('====')\n",
    "    print('\\t'.join(list(comm_strings.groups[x])))\n",
    "    #print(', '.join(list(x.index())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sector lookup\n",
    "\n",
    "sector_labels = ['industrial','ads','aerospace','food_agriculture','recruitment',\n",
    "'data_analytics','apps','finance','content','construction_real_state',\n",
    "'non_profit','immersive','transport',\n",
    "'b2b','energy','retail_fashion','security','health','marketing','innovation','education',\n",
    "'ict','marketplace_sharing_economy','telecommunications','computing','legal_professional',\n",
    "'consumer_services','software_development','sales','sports_games','events','travelling','information',\n",
    "'internet','smart_sensors','search','social_networks','adult']\n",
    "\n",
    "#They are in reverse order because I labelled them from below\n",
    "industry_lookup = {n:y for n,y in enumerate(sector_labels[::-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these categories may be too aggregate - eg sports games contain both video games (creative) and sports (non creative).\n",
    "\n",
    "# One way to deal with this is by increasing the granularity of the community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lookup every category\n",
    "comps_cats['sector_list']= [[industry_lookup[comms[lab]] for lab in cats] for cats in comps_cats['category_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_cats['sector_list'].head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of overlap between categories - hard to delineate companies into a single category. For now we will perform an analysis that considers all sectors for a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_counts = []\n",
    "\n",
    "for sector in set(industry_lookup.values()):\n",
    "    \n",
    "    #Subset the df to find all companies that mention sector\n",
    "    has_sector = comps_cats.loc[[sector in indust for indust in comps_cats['sector_list']]]\n",
    "    \n",
    "    #How many are there?\n",
    "    sector_tot = len(has_sector)\n",
    "    \n",
    "    #How many companies mentioning AI in the category?\n",
    "    ai_tot = has_sector['ai_flag'].sum()\n",
    "    \n",
    "    #Create a series\n",
    "    out = pd.Series([sector_tot,ai_tot],name=sector)\n",
    "    \n",
    "    ai_counts.append(out)\n",
    "    \n",
    "#Create df and label\n",
    "ai_sector_df = pd.concat(ai_counts,axis=1).T\n",
    "ai_sector_df.columns = ['total','ai']\n",
    "\n",
    "#Create a 'share' variable showing the proportion of ai companies in a vertical\n",
    "ai_sector_df['shares'] = 100*(ai_sector_df['ai']/ai_sector_df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ai_sector_df.sort_values('shares',ascending=False)['shares'].plot.bar(figsize=(9,5),color='coral',edgecolor='black')\n",
    "ax.set_ylabel('AI companies as \\n % of total',size=14)\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.set_title('AI representation in CrunchBase verticals',size=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jmateosgarcia/Desktop/company_focus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many AI companies in creative sectors in the UK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_sector = ['ads','immersive','marketing','content',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_finance = comps_cats.loc[(comps_cats['country']=='United Kingdom')&(['finance' in x for x in comps_cats['sector_list']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(uk_finance)) \n",
    "print(uk_finance['ai_flag'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_drop = ['permalink','domain','email','phone','facebook_url','linkedin_url','logo_url','is_health','mesh_terms','organization',\n",
    "               'parent_id','organization_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_comps = comps_cats.loc[(comps_cats['country']=='United Kingdom'),[x for x in comps_cats.columns if x not in vars_to_drop]]\n",
    "uk_comps.to_csv(f'{today_str}_cb_uk.csv',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('|name|type|observations|')\n",
    "print('|----|----|----|')\n",
    "\n",
    "for c in uk_comps.columns:\n",
    "    \n",
    "    print(f'|{c}|{type(uk_comps[c].iloc[0])}|   |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
