{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv keyword search\n",
    "\n",
    "We have attempted two supervised models to classify arXiv projects into creative industries without success.\n",
    "\n",
    "Having given up on that stream of analysis, here we adopt a keyword-based search instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions etc here\n",
    "\n",
    "import random\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return([x for el in a_list for x in el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx = pd.read_csv('../data/processed/6_8_2019_arxiv_processed.csv',compression='zip')\n",
    "\n",
    "arx_papers = arx.drop_duplicates('article_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and tokenise the arXiv data\n",
    "arx_papers['summary'] = [re.sub('\\n',' ',x) for x in arx_papers['summary']]\n",
    "\n",
    "arxiv_pro = CleanTokenize(arx_papers['summary']).clean().bigram().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use word2vec\n",
    "w2v_arx = Word2Vec(arxiv_pro.tokenised,window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_sector_seed = {\n",
    "    'Advertising and marketing':['advertising','advert','marketing',\n",
    "                                 'search_engine','social_media'],\n",
    "    \n",
    "    'Architecture':['smart_city','traffic','transportation','house'],\n",
    "     #'Crafts':\n",
    "    \n",
    "    'immersive_games': ['video_game','virtual_reality','augmented_reality'],\n",
    "    \n",
    "    'Design':['user_interface','user_interaction','usability'],\n",
    "     \n",
    "    'Film, TV, video, radio and photography':['video','film','television','radio','photography','image','photo'],\n",
    "    \n",
    "    'IT, software and computer services':['software','programming','software_development'],\n",
    "    \n",
    "    'Museums, galleries and libraries':['museum','gallery','archive','exhibition'\n",
    "                                        #'library','repository'\n",
    "                                       ],\n",
    "    \n",
    "    'Music, performing and visual arts':['music','song','audio','art','artist','creativity','theatre'],\n",
    "    \n",
    "    'Publishing': ['printing','book','e-book','book','journalism','newspaper']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({k:', '.join(v) for k,v in creative_sector_seed.items()},index=['Keywords']).T.to_csv('../data/external/arxiv_kw_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dict comprehension extracts similar terms to the one above, but only if they are above a certain similarity threshold\n",
    "\n",
    "#Here we go.\n",
    "expanded_kws = {k:set(\n",
    "    flatten_list([[x[0] for x in w2v_arx.wv.most_similar(w) if x[1]>0.8] for w in v])+\n",
    "    [w for w in k.lower().split(' ') if w not in stop]+[w for w in v]) for k,v in \n",
    "    creative_sector_seed.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_terms(kw_set,drop):\n",
    "    '''\n",
    "    Function that drops some irrelevant term from the expanded keyword set\n",
    "    '''\n",
    "    \n",
    "    kw_set_2 = {k:set([val for val in v if val not in drop]) for k,v in kw_set.items()}\n",
    "    \n",
    "    return(kw_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afer checking the results, these are the terms that we drop\n",
    "kw_set_2 = drop_terms(expanded_kws,['it,','package','python','services','thinking','charles','fabricate','full-text','greek',\n",
    "              'lecture','lectures','proceedings','russian','turkish','talks','wikipedia_articles','library','repository',\n",
    "                                   'architecture','design','designer','finnish','politicians','politics'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we label papers with term occurrences\n",
    "\n",
    "out = []\n",
    "\n",
    "#This loops over each list of keywords and counts their occurrences in the tokenised version of the corpus\n",
    "for k,val in creative_sector_seed.items():\n",
    "    \n",
    "    \n",
    "    #Count the number of occurrences of keywords related to a creative industry in a paper\n",
    "    res = [len(kw_set_2[k]&set(abst)) for abst in arxiv_pro.tokenised]\n",
    "    out.append([k,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a dataframe with the outputs that we stored in the out list above\n",
    "#Note that we create a prefix for the name to make things easier to read later\n",
    "\n",
    "out_labels = pd.DataFrame({'kw_n_'+re.sub(' ','_',x[0].lower()):x[1] for x in out})\n",
    "\n",
    "out_labels.sum()\n",
    "\n",
    "sect_labels = out_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate into a single dataframe\n",
    "arx_pred = pd.concat([arx_papers,out_labels],axis=1)\n",
    "\n",
    "arx_pred['year'] = [int(x.split('-')[0]) for x in arx_pred['article_created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheap check of results\n",
    "\n",
    "for s in out_labels.columns:\n",
    "    \n",
    "    print(s)\n",
    "    print('===')\n",
    "        \n",
    "    arx_temp = arx_pred.loc[arx_pred[s]>1]\n",
    "    \n",
    "    \n",
    "    choose_three = random.sample(list(arx_temp['summary']),5)\n",
    "    \n",
    "    for d in choose_three:\n",
    "        \n",
    "        print(d)\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of AI papers per 'sector area'\n",
    "\n",
    "ai_in_sector = pd.concat([pd.crosstab(arx_pred['ai'],arx_pred[s]>1)[True] for s in sect_labels],axis=1)\n",
    "ai_in_sector.columns = sect_labels\n",
    "\n",
    "ai_in_sector.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(figname,path='../reports/figures/figures_report/'):\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+figname+'.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levels and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_sectors = sect_labels\n",
    "\n",
    "sect_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sector_distribution(df,ai_var,sector_names,thres,norm=False):\n",
    "    '''\n",
    "    Create a table of AI activity per sector.\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe where the rows are entities and the columns fields of interest\n",
    "        ai_var is the variable with AI\n",
    "        sector_names are the names of the sectors (we assume that the sectors are columns)\n",
    "        thres is the value in the sector column indicating presence of CI\n",
    "        norm is whether we want to normalise the output or nor\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the distribution\n",
    "    sector_distr = pd.concat([df.loc[df[sector]>thres][ai_var].value_counts() for sector in sector_names],axis=1).T\n",
    "\n",
    "    #Prettify the variable names\n",
    "    sector_distr.index = [' '.join(x.split('_')[2:]).capitalize() for x in sector_names]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Do we normalise?\n",
    "    if norm==False:\n",
    "        return(sector_distr)\n",
    "    #Normalise over cols\n",
    "    elif norm==1:\n",
    "        return(sector_distr.apply(lambda x: x/x.sum(),axis=1))\n",
    "    else:\n",
    "        return(sector_distr.apply(lambda x: x/x.sum(),axis=0))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_ai_act = create_sector_distribution(arx_pred,'ai',creative_sectors,thres=1)\n",
    "creative_ai_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_path = '../reports/figures/figures_report/arx_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sector_distribution(arx_pred,'ai',creative_sectors,thres=1,norm=1).sort_values(True,ascending=True)[\n",
    "    True].plot.barh(title='Share of AI papers in CI category',figsize=(6,4))\n",
    "\n",
    "\n",
    "save_fig('shares',arx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sectors = creative_ai_act.sort_values(True,ascending=False).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sector overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = arx_pred[creative_sectors]>1\n",
    "\n",
    "pd.crosstab(overlaps['kw_n_advertising_and_marketing'],overlaps['kw_n_film,_tv,_video,_radio_and_photography'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_analysis(df,year_var,ai_var,sector_names,thres,norm=False,year_lims=False):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function to create table of research trends\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe where the rows are entities and the columns fields of interest\n",
    "        year_var is the variable with the year\n",
    "        ai_var is the variable with AI\n",
    "        sector_names are the names of the sectors (we assume that the sectors are columns)\n",
    "        thres is the value in the sector column indicating presence of CI\n",
    "        norm is whether we want to normalise the output or nor\n",
    "        year_lims is the variable with the year limits (min and max)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if norm == False:\n",
    "        \n",
    "        sector_year = pd.concat([\n",
    "            df.loc[df[s]>thres].groupby([year_var,ai_var]).size().reset_index(\n",
    "                drop=False).pivot_table(index=year_var,columns=ai_var,values=0).fillna(0)[True] for s in sector_names],\n",
    "            axis=1).fillna(0)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        sector_year = pd.concat([\n",
    "            df.loc[df[s]>thres].groupby([year_var,ai_var]).size().reset_index(\n",
    "                drop=False).pivot_table(index=year_var,columns=ai_var,values=0).fillna(\n",
    "                0).apply(lambda x: x/x.sum(),axis=1)[True] for s in sector_names],axis=1).fillna(0)\n",
    "        \n",
    "        \n",
    "         \n",
    "    #sector_year = pd.concat([df.loc[(df[sect]>thres)&(df[ai_var]==True)][year_var].value_counts() \n",
    "    #                         for sect in sector_names],axis=1).fillna(0)\n",
    "                               \n",
    "    if year_lims != False:\n",
    "        \n",
    "        sector_year = sector_year.loc[(sector_year.index>year_lims[0])&(sector_year.index<year_lims[1])]\n",
    "\n",
    "    sector_year.columns = [' '.join(x.split('_')[2:]).capitalize() for x in sector_names]\n",
    "    \n",
    "    return(sector_year)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_total = trend_analysis(arx_pred,'year','ai',creative_sectors,thres=1,year_lims=[2000,2019],norm=0)\n",
    "\n",
    "ax = trends_total[top_sectors].plot.bar(stacked=True,title='Paper Totals',figsize=(8,4))\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('sector_years',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare growth in CI AI vs all activity and other AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_trends = pd.crosstab(arx_pred['year'],arx_pred['ai']).loc[np.arange(2001,2019)].apply(lambda x: x/x.sum())\n",
    "\n",
    "ai_trends.columns = ['Not AI','All AI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ai_trends.columns)+list(top_sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_shares = trends_total.apply(\n",
    "    lambda x: x/x.sum(),axis=0)\n",
    "\n",
    "all_sectors = pd.concat([ai_trends,trend_shares],axis=1)[list(ai_trends.columns)+list(top_sectors)]\n",
    "\n",
    "all_sectors.rolling(window=5).mean().dropna().plot(linewidth=2,figsize=(8,5),title='Share of year in sector activity',cmap='Dark2_r')\n",
    "\n",
    "save_fig('trends_share_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = trends_total.apply(lambda x: x/x.sum(),axis=1)[top_sectors].rolling(window=3).mean().dropna().plot.bar(stacked=True,width=0.8,\n",
    "                                                                                                           title='Sector shares of total',figsize=(8,4))\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('sector_years_shares',arx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_mean = trend_analysis(arx_pred,'year','ai',creative_sectors,thres=1,year_lims=[1997,2019],norm=1)\n",
    "\n",
    "ax = trends_mean[top_sectors].rolling(window=5).mean().dropna().plot(linewidth=3,title='AI as share of all papers in sector',figsize=(8,4))\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('sector_years_share_of_sector',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography\n",
    "\n",
    "Here, we will need to re-merge the papers with the initial geo-coded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lq_df(df):\n",
    "    '''\n",
    "    Takes a df with cells = activity in col in row and returns a df with cells = lq\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    area_activity = df.sum(axis=0)\n",
    "    area_shares = area_activity/area_activity.sum()\n",
    "    \n",
    "    lqs = df.apply(lambda x: (x/x.sum())/area_shares, axis=1)\n",
    "    return(lqs)\n",
    "\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we merge the arx enriched dataset with the \n",
    "arx_ci_geo = pd.merge(arx,arx_pred[['article_id','year']+list(sect_labels)],left_on='article_id',right_on='article_id')\n",
    "\n",
    "#arx_ci_geo['year'] = [int(x.split('-')[0]) for x in arx_ci_geo['year_created']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the multinationals\n",
    "arx_ci_geo_nom = arx_ci_geo.loc[arx_ci_geo['is_multinational']==0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_ci_geo_nom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a df with individual country - sector\n",
    "\n",
    "def geo_analysis(df,ai_var,geo_var,sector_names,threshold,top_c=False):\n",
    "    '''\n",
    "    \n",
    "    Create distributions of activity by sector and geography\n",
    "    \n",
    "    Args:\n",
    "        -df is the table with the information, every row is an entity with geographical information, ai information, sector information etc\n",
    "        -ai_var is the ai variable\n",
    "        -geo_var is the variable with the countries\n",
    "        -sector_names is the sectors we want to extract information for\n",
    "        -threshold is the threshold above which we accept a paper as 'creative'\n",
    "        -top_c is the number of countries we want to output\n",
    "    \n",
    "    Outputs:\n",
    "        -Table with counts of papers by country and sector. We can normalise later.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Table\n",
    "    country_dist = pd.concat([df.loc[(df[ai_var]==True)&(df[s]>threshold)][geo_var].value_counts() for s in sector_names],axis=1).fillna(0)\n",
    "    \n",
    "    country_dist.columns = [' '.join(x.split('_')[2:]).capitalize() for x in sector_names]\n",
    "    \n",
    "    country_dist = country_dist.loc[country_dist.sum(axis=1).sort_values(ascending=False).index]\n",
    "    \n",
    "    if top_c!=False:\n",
    "        \n",
    "        country_dist = country_dist.iloc[top_c]\n",
    "    \n",
    "    return(country_dist)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = geo_analysis(arx_ci_geo_nom,'ai','institute_country',creative_sectors,threshold=1)\n",
    "\n",
    "ax = geo.iloc[:15][top_sectors].plot.bar(stacked=True,title='Papers by sector and country',figsize=(8,4))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('country_totals',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market 'shares'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = geo.apply(lambda x: x/x.sum(),axis=0)[top_sectors[::-1]].iloc[:15].T.plot.barh(\n",
    "    stacked=True,cmap='tab20',edgecolor='lightgrey',figsize=(9,7),width=0.8,title='Research market share by country')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('copuntry_market_shares',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sector_year_trend(df,year_var,geo_var):\n",
    "    '''\n",
    "    Creates a table with number of papers by country and year\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    country_year = pd.crosstab(df[year_var],df[geo_var])\n",
    "    \n",
    "    return(country_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_name_mapping = {' '.join(x.split('_')[2:]).capitalize():x for x in creative_sectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_trends(df,ai_var,year_var,geo_var,sector_names,threshold,top_c=False,year_lims= False):\n",
    "    \n",
    "    '''\n",
    "    Visualise geotrends by sector\n",
    "    \n",
    "    Creates a set of tables for each sector with number of papers by country and year\n",
    "    \n",
    "    args:\n",
    "    \n",
    "        -df is the table with the information, every row is an entity with geographical information, ai information, sector information etc\n",
    "        -ai_var is the ai variable\n",
    "        -geo_var is the variable with the countries\n",
    "        -sector_names is the sectors we want to extract information for\n",
    "        -threshold is the threshold above which we accept a paper as 'creative'\n",
    "        -top_c is the number of countries we want to output\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #We focus on ai papers\n",
    "    \n",
    "    ai_papers = df.loc[df[ai_var]==True]\n",
    "    \n",
    "    #We store the sectoral results here\n",
    "    sector_store = {}\n",
    "    \n",
    "    #For each sector\n",
    "    for s in sector_names:\n",
    "        \n",
    "        #Calculate number of papers in sector by year and country\n",
    "        \n",
    "        out = make_sector_year_trend(ai_papers.loc[ai_papers[s]>threshold],year_var,geo_var).fillna(0)\n",
    "        \n",
    "        #This is to focus on popular countries (if we want)\n",
    "        if top_c!=False:\n",
    "            top_cs = out.sum(axis=0).sort_values(ascending=False).index[:top_c]\n",
    "            out = out[top_cs]\n",
    "            \n",
    "        #This is to limit the number of years we focus on\n",
    "        if year_lims!=False:\n",
    "            \n",
    "            out = out.loc[(out.index>year_lims[0]) & (out.index<year_lims[1])]\n",
    "        \n",
    "        \n",
    "        #out['sector']=s\n",
    "        sector_store[s]=out\n",
    "    \n",
    "    return(sector_store)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trends = arx_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_trends = geo_trends(arx_ci_geo,'ai','year','institute_country',creative_sectors,threshold=1,top_c=5,year_lims=[2000,2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=len(creative_sectors),figsize=(10,18))\n",
    "\n",
    "for n,s in enumerate(top_sectors):\n",
    "    \n",
    "    sector_trends[sector_name_mapping[s]].dropna().plot(ax=ax[n],title=s)\n",
    "    \n",
    "    #print(sector_trends[sector_name_mapping[s]].rolling(window=4).mean())\n",
    "    \n",
    "    ax[n].legend(bbox_to_anchor=(1,1),ncol=2)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate UK shares of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_trends_country(df,ai_var,year_var,geo_var,sector_names,threshold,country,top_c=False,year_lims= False):\n",
    "    \n",
    "    '''\n",
    "    Visualise geotrends by sector\n",
    "    \n",
    "    Creates a set of tables for each sector with number of papers by country and year\n",
    "    \n",
    "    args:\n",
    "    \n",
    "        -df is the table with the information, every row is an entity with geographical information, ai information, sector information etc\n",
    "        -ai_var is the ai variable\n",
    "        -geo_var is the variable with the countries\n",
    "        -sector_names is the sectors we want to extract information for\n",
    "        -threshold is the threshold above which we accept a paper as 'creative'\n",
    "        -top_c is the number of countries we want to output\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #We focus on ai papers\n",
    "    \n",
    "\n",
    "    \n",
    "    ai_papers = df.loc[df[ai_var]==True]\n",
    "    \n",
    "    #We store the sectoral results here\n",
    "    sector_store = {}\n",
    "    \n",
    "    #Also for all papers (as a benchmark)\n",
    "    \n",
    "    all_papers = make_sector_year_trend(df,year_var,geo_var)\n",
    "    \n",
    "    all_papers_shares = all_papers.apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    sector_store['All'] = all_papers_shares[country]\n",
    "    \n",
    "    #For each sector\n",
    "    for s in sector_names:\n",
    "        \n",
    "        #Calculate number of papers in sector by year and country\n",
    "        \n",
    "        out = make_sector_year_trend(ai_papers.loc[ai_papers[s]>threshold],year_var,geo_var).fillna(0)\n",
    "        \n",
    "        \n",
    "        out_shares = out.apply(lambda x: x/x.sum(),axis=1)\n",
    "        \n",
    "        #This is to limit the number of years we focus on\n",
    "        if year_lims!=False:\n",
    "            \n",
    "            out_shares = out_shares.loc[(out_shares.index>year_lims[0]) & (out_shares.index<year_lims[1])]\n",
    "        \n",
    "        \n",
    "        #out['sector']=s\n",
    "        sector_store[s]=out_shares[country]\n",
    "    \n",
    "    return(sector_store)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = pd.DataFrame(geo_trends_country(arx_ci_geo_nom,'ai','year','institute_country',creative_sectors,0,'United Kingdom',year_lims=[2000,2019])).fillna(0)\n",
    "\n",
    "uk.columns =  ['All research']+[' '.join(x.split('_')[2:]).capitalize() for x in uk.columns[1:]]\n",
    "\n",
    "ax = (100*uk.rolling(window=5).mean()).dropna().plot(linewidth=2,title='Research market share of UK papers',figsize=(8,4))\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "save_fig('country',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialisation (discretised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_specialisation_table(geo_df,top):\n",
    "    '''\n",
    "    Creates a specialisation table.\n",
    "    \n",
    "    Args:\n",
    "        geo df is a table with geographical units in the rows and sector activity in the columns\n",
    "        top is the number of geographical units to report\n",
    "        \n",
    "    Returns:\n",
    "        A table for a heatmap visualisation\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    lqs = create_lq_df(geo_df)\n",
    "    \n",
    "    #Select, arrange and rank\n",
    "    select = lqs.iloc[:top].apply(lambda x: pd.qcut(x,np.arange(0,1.1,0.2),labels=False,duplicates='drop'),axis=1)\n",
    "    \n",
    "    return(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ai_country = pd.crosstab(arx_ci_geo_nom['institute_country'],arx_ci_geo_nom['ai'])\n",
    "\n",
    "all_ai_country.columns = ['Non AI','all_AI']\n",
    "\n",
    "ai_non_ci = arx_ci_geo_nom.loc[(arx_ci_geo_nom[creative_sectors].max(axis=1)<2)&(arx_ci_geo_nom['ai']==True)]\n",
    "\n",
    "ai_non_ci_country = pd.crosstab(ai_non_ci['institute_country'],ai_non_ci['ai'])\n",
    "\n",
    "ai_non_ci_country.columns = ['AI non CI']\n",
    "\n",
    "geo_all = pd.concat([all_ai_country['Non AI'],ai_non_ci_country['AI non CI'],geo],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_all_sorted = geo_all.loc[geo_all[top_sectors].sum(axis=1).sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(geo_all_sorted.columns[:2])+list(top_sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "ax = sn.heatmap(make_specialisation_table(geo_all_sorted[list(geo_all_sorted.columns[:2])+list(top_sectors)],15).T,cmap='Oranges',\n",
    "          edgecolor='lighgrey',linewidth=0.01,ax=ax)\n",
    "\n",
    "ax.set_title('Country Research specialisation')\n",
    "\n",
    "ax.collections[0].colorbar.set_label(\"Specialisation quartile\")\n",
    "\n",
    "save_fig('sector_specs',arx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institutional analysis\n",
    "\n",
    "Here we create a df where one of the fields is a list with the institutions involved.\n",
    "\n",
    "We will use this to identify the most active organisations by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_inst = arx_ci_geo.copy().dropna(axis=0,subset=['institute_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up the institution names to remove duplicated matches \n",
    "arx_inst['institute_ded'] = [x.split('(')[0] for x in arx_inst['institute_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by paper and drop duplicates\n",
    "\n",
    "arx_inst_unique = arx_inst.drop_duplicates('article_id')[[x for x in arx_inst.columns if not any(var in x for var in ['country','state','lat','lon','city','multinational','inst'])]]\n",
    "\n",
    "inst_per_paper = arx_inst.groupby('article_id')['institute_ded'].apply(lambda x: list(set(x))).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the institution lists to the papers\n",
    "arx_inst_unique['institute_list'] = [inst_per_paper[x] for x in arx_inst_unique['article_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inst_counts(df,inst_var):\n",
    "    '''\n",
    "    Counts institutions in a df\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = pd.Series(flatten_list(df[inst_var])).value_counts()\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_institutions(df,ai_var,inst_var,year_var,sector_names,threshold,top_insts=5,year_lims=[2008,2019]):\n",
    "    '''\n",
    "    Extracts the top institutions in each sector and the evolution of concentration\n",
    "    \n",
    "    Args:\n",
    "        -df is the table with the information, every row is an entity with institutional information, ai information, sector information etc\n",
    "        -ai_var is the ai variable\n",
    "        -inst var is the variable with the institutions\n",
    "        -sector_names is the sectors we want to extract information for\n",
    "        -threshold is the threshold above which we accept a paper as 'creative' in a sector\n",
    "        -top_c is the number of institutes we want to output\n",
    "\n",
    "    Returns a table with top institutions and the evolution of concentration over time\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Get the totals for each sector\n",
    "    \n",
    "    out = {}\n",
    "    \n",
    "    for s in sector_names:\n",
    "        \n",
    "        #Append the total papers per institution\n",
    "        tot = make_inst_counts(df.loc[(df[ai_var]==True)& (df[s]>threshold)],inst_var).sort_values(ascending=False)\n",
    "        \n",
    "        #top_inst_store[s] = tot.iloc[:top_insts]\n",
    "        \n",
    "        #Get the yearly activity\n",
    "        \n",
    "        #Combine by year\n",
    "        tot_year = pd.concat([make_inst_counts(df.loc[(df[ai_var]==True)& (df[s]>threshold) & (df[year_var]==y)],inst_var).reset_index(drop=True).sort_values(ascending=False) for y in \n",
    "                              np.arange(year_lims[0],year_lims[1])],axis=1).fillna(0)\n",
    "        \n",
    "        \n",
    "        tot_year.columns = np.arange(year_lims[0],year_lims[1])\n",
    "        \n",
    "        #print(tot_year)\n",
    "        \n",
    "        top_x = tot_year.apply(lambda x: x/x.sum()).iloc[:top_insts].sum(axis=0)\n",
    "        \n",
    "        #print(top_x.iloc[:top_insts].sum(axis=0))\n",
    "        \n",
    "        \n",
    "        out[s] = [tot.iloc[:top_insts],top_x]\n",
    "            \n",
    "    return(out)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in sector_names:\n",
    "        \n",
    "#         #Append the total papers per institution\n",
    "#         tot = make_inst_counts(df.loc[(df[ai_var]==True)& (df[s]>threshold)],inst_var).sort_values(ascending=False)\n",
    "        \n",
    "#         top_inst_store.append(tot.iloc[:top_insts])\n",
    "        \n",
    "#         #Get the yearly activity\n",
    "        \n",
    "#         #Combine by year\n",
    "#         tot_year = pd.concat([make_inst_counts(df.loc[(df[ai_var]==True)& (df[s]>threshold) & (df[year_var]==y)],inst_var).sort_values(ascending=False) for y in \n",
    "#                               np.arange(year_lims[0],year_lims[1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = top_institutions(arx_inst_unique,'ai','institute_list','year',creative_sectors,1,top_insts=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=len(creative_sectors),figsize=(8,18))\n",
    "\n",
    "for n,s in enumerate(top_sectors):\n",
    "    \n",
    "    #inst[sector_name_mapping[s]][0].plot.barh(ax=ax[n][0],title=s)\n",
    "    \n",
    "    inst[sector_name_mapping[s]][1].plot(ax=ax[n],title=s)\n",
    "    \n",
    "    \n",
    "    #ax[n].legend(bbox_to_anchor=(1,1),ncol=2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('top_institutions',arx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_ci_geo_nom.loc[arx_ci_geo_nom['institute_country']=='United Kingdom'].to_csv(f'../data/processed/{today_str}_arxiv_uk_papers.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arx_pred.loc[arx_pred[creative_sectors].max(axis=1)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arx_pred.loc[(arx_pred[creative_sectors].max(axis=1)>1) & (arx_pred['ai']==True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5017/12517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arx_pred.loc[(arx_pred['ai']==True)])/len(arx_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*5000/arx_pred['ai'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in creative_sectors:\n",
    "    \n",
    "#     print(s)\n",
    "#     print('===')\n",
    "    \n",
    "#     summ = arx_pred.aloc[(arx_pred[s]>2)&(arx_pred['ai']==True)].sort_values('citation_count',ascending=False)['summary'].iloc[0]\n",
    "#     print(summ)\n",
    "    \n",
    "#     print('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arx_ci_geo_nom.loc[(arx_ci_geo_nom[creative_sectors].max(axis=1)>1)&(arx_ci_geo_nom['institute_country']=='United States')])/12157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(arx_ci_geo_nom.loc[(arx_ci_geo_nom[creative_sectors].max(axis=1)>1)&(arx_ci_geo_nom['institute_country']=='China')])/12157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*len(arx_ci_geo_nom.loc[(arx_ci_geo_nom[creative_sectors].max(axis=1)>1)&(arx_ci_geo_nom['institute_country']=='United Kingdom')])/12157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling\n",
    "\n",
    "We will use a stochastic block model topic modelling approach that automatically selects the number of topics and displays them as a hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports\n",
    "\n",
    "#Imports\n",
    "from sbmtm import sbmtm\n",
    "import graph_tool.all as gt\n",
    "\n",
    "def make_document_topic_df(model,level,n_words):\n",
    "    '''\n",
    "    \n",
    "    We extract a document-topic df from the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: model object\n",
    "        level (int): level of the model at which we want to extract the topics\n",
    "        n_words: number of words we want to use to label the columns in the document-topic df\n",
    "        \n",
    "    Outputs:\n",
    "        A document topic df where every row is a paper (with its id) and each column is the the weight for a topic. The columns are labelled with the topic names\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the topic mix\n",
    "    d_t_df = pd.concat([pd.DataFrame(model.topicdist(n,l=level),columns=['topic',pid]).set_index('topic') for \n",
    "                      n,pid in enumerate(model.documents)],axis=1).T\n",
    "    \n",
    "    #Create the columns\n",
    "    topic_names = ['-'.join([x[0] for x in topic_comp][:n_words]) for topic_comp in model.topics(l=level).values()]\n",
    "    \n",
    "    d_t_df.columns = topic_names\n",
    "    \n",
    "    #We name the index to simplify merging later\n",
    "    d_t_df.index.name = 'paper_id'\n",
    "    \n",
    "    return(d_t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe that contains only the AI papers. We can use this for the topic modelling\n",
    "\n",
    "arx_ci_ai = arx_pred.loc[(arx_pred[sect_labels].sum(axis=1)>1) & (arx_pred['ai']==True)]\n",
    "\n",
    "arx_ci_indices = set(arx_ci_ai.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the bag of words abstracts for AI papers\n",
    "arx_ci_ai_tok = [x for n,x in enumerate(arxiv_pro.tokenised) if n in arx_ci_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## we create an instance of the sbmtm-class\n",
    "model = sbmtm()\n",
    "\n",
    "## we have to create the word-document network from the corpus\n",
    "model.make_graph(arx_ci_ai_tok,\n",
    "                 documents=list(arx_ci_ai['article_id']))\n",
    "\n",
    "## fit the model\n",
    "gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix = make_document_topic_df(model,0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
