{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway to Research\n",
    "\n",
    "This notebook loads and shows the Gateway to Research data\n",
    "\n",
    "Check this [repo](https://github.com/nestauk/gtr_data_processing) for additional information about the GtR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions etc here\n",
    "import re\n",
    "from pylab import *\n",
    "from plotnine import * \n",
    "import geopandas as gpd\n",
    "from string import punctuation\n",
    "from pyproj import Proj\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return([x for el in a_list for x in el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in the data that has been processed for university effects\n",
    "\n",
    "    \n",
    "    \n",
    "my_path = 'filepath/060819_gtr_creative_sect.csv'\n",
    "\n",
    "\n",
    "gtr = pd.read_csv(my_path,compression='zip',na_values='[]').iloc[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr['creative_sector'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a flag for all the categories and individual flags to handle the individual components i.e.\n",
    "\n",
    " a=['Museums, galleries and libraries', 'Film, TV, video, radio and photography' 'Design','Architecture','Publishing' , 'Advertising and marketing','Crafts', 'IT,software and computer services', 'Music, performing and visual arts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_industry=['Museums, galleries and libraries', 'Film, TV, video, radio and photography' 'Design','Architecture','Publishing' , 'Advertising and marketing','Crafts', 'IT, software and computer services', 'Music, performing and visual arts']\n",
    "\n",
    "\n",
    "#General creative function\n",
    "def creativesearch(x):\n",
    " regex = re.compile(\"|\".join(word for word in creative_industry), re.IGNORECASE)\n",
    " if regex.search(x):\n",
    "    return 1 #This is done as you can't subset dataset with None and not equals operator\n",
    " else:\n",
    "   return 0\n",
    "\n",
    "#Domain function\n",
    "def domain(x,y): # y is the word x is the column it is applied to\n",
    " regex = re.compile(y, re.IGNORECASE)\n",
    " if regex.search(x):\n",
    "    return 1 #This is done as you can't subset dataset with None and not equals operator\n",
    " else:\n",
    "   return 0\n",
    "\n",
    "\n",
    "#Set as string\n",
    "gtr[['creative_sector']]=gtr[['creative_sector']].astype(str)\n",
    "\n",
    "\n",
    "#Apply functions\n",
    "gtr['creative_flag']=gtr[['creative_sector']].applymap(creativesearch)\n",
    "\n",
    "#Creates sector flags for each category\n",
    "for elem in creative_industry:\n",
    " gtr[elem]=gtr[['creative_sector']].applymap(lambda x:domain(x, elem))\n",
    " \n",
    "gtr.head(n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the count of ai by the different creative sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sums the dataframe by AI status\n",
    "countby_ai_status=gtr.groupby(['ai_mod']).sum()\n",
    "\n",
    "#Drops most of the variables, except the ones we want\n",
    "countby_ai_status=countby_ai_status[creative_industry+['creative_flag']]\n",
    "\n",
    "#Pastes to clipboard\n",
    "countby_ai_status.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=countby_ai_status.loc[True , : ].plot.bar(figsize=(10,5))\n",
    "ax.set_ylabel('Number of AI related projects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_the_abstracts=gtr['abstract'][(gtr['ai_mod']==True) &  (gtr['creative_flag']==1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks at how the number of projects is changing over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(ggplot(gtr[gtr['ai_mod']==True],aes(x='year',group='ai_mod',color='ai_mod'))+\n",
    "  geom_freqpoly(binwidth = 1, show_legend=False) +xlab(\"Year\")+ylab(\"Number of AI projects\")+xlim(2007,2018)+ylim(0,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Was creative_flag_semantic\n",
    "(ggplot(gtr[gtr['creative_flag']==True],aes(x='year',group='creative_flag',color='creative_flag'))+\n",
    "  geom_freqpoly(binwidth = 1, show_legend=False) +xlab(\"Year\")+ylab(\"Number of Creative projects\")+xlim(2007,2018)+ylim(0,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI and Creative projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(gtr[(gtr['creative_flag']==True) & (gtr['ai_mod']==1)],aes(x='year'))+\n",
    "  geom_freqpoly(binwidth = 1, show_legend=False) +xlab(\"Year\")+ylab(\"Number of AI and Creative projects\")+xlim(2007,2018)+ylim(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads data\n",
    "\n",
    "stem=\"filepath\"\n",
    "\n",
    "files=\"Local_Authority_Districts_December_2017_Super_Generalised_Clipped_Boundaries_in_United_Kingdom_WGS84.shp\"\n",
    "\n",
    "#proje=\"+proj=utm +zone=33 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "UK_lad=gpd.read_file(stem+files)\n",
    "\n",
    "UK_lad.crs\n",
    "\n",
    "#Sets the projection\n",
    "\n",
    "#UK_lad = UK_lad.to_crs({'init' :'epsg:25832'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Issue in the projection to resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check it's loaded\n",
    "ax=UK_lad.plot( figsize=(5, 5))\n",
    "ax.set_title('')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Does spatial counts of local authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorts out the multiple local authorities\n",
    "\n",
    "#subsets the data so ai and creative only\n",
    "creative_ai=gtr[(gtr['ai_mod']==True) &  (gtr['creative_flag']==1)]\n",
    "\n",
    "creative_ai.shape\n",
    "\n",
    "\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "creative_ai['all_lad_code']=creative_ai['all_lad_code'].astype(str)\n",
    "\n",
    "creative_ai['all_lad_code']=creative_ai['all_lad_code'].map(strip_punctuation)\n",
    "\n",
    "\n",
    "#Convert the dataframe of lists into one single list\n",
    "\n",
    "#concatenate the strings\n",
    "a=''\n",
    "    \n",
    "for elem in creative_ai['all_lad_code']:\n",
    "     a=a+' '+str(elem)\n",
    "# split them to get a list\n",
    "a=a.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does a table of the number of local authorities in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Count the elements of the dataframe\n",
    "    \n",
    "d=Counter(a)\n",
    "\n",
    "#Convert the counter to a dataframe\n",
    "ai_creative_count = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "\n",
    "\n",
    "#sort out the column names\n",
    "ai_creative_count.rename(columns={'index':'la_code', 0:'project count'}, inplace=True)\n",
    "\n",
    "ai_creative_count.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merges the two datasets\n",
    "UK_lad=UK_lad.merge(ai_creative_count, how='left', left_on='lad17cd'  , right_on='la_code')\n",
    "\n",
    "UK_lad.tail(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Authority map for all participating organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_lad['project count']=UK_lad['project count'].fillna(0)\n",
    "\n",
    "\n",
    "ax=UK_lad.plot(column='project count', cmap='cool', figsize=(15,15))\n",
    "ax.set_title('')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of local authorities count for all participating organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs=UK_lad[['lad17nm' ,'project count']].sort_values(by='project count', ascending=False)\n",
    "\n",
    "#set as integer\n",
    "tabs['project count']=tabs['project count'].astype(int)\n",
    "\n",
    "#renames the columns\n",
    "tabs.rename(columns={'lad17nm':'local authority', 'project count':'project partner count'}, inplace=True)\n",
    "\n",
    "#drops the index\n",
    "tabs=tabs.reset_index(drop=True)\n",
    "\n",
    "tabs.head(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analysis of the data at the intersection of AI and creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn import metrics  #for the cluster metrics like silhoute score\n",
    "from sklearn import manifold #for TSNE\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "from time import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the data, admittedly a small sample\n",
    "\n",
    "df=gtr[(gtr['ai_mod']==True) &  (gtr['creative_flag']==1)  ]\n",
    "\n",
    "column_names = ['abstract']\n",
    "\n",
    "df[column_names].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Sets to lower case\n",
    "df[column_names] = df[column_names].applymap(lambda x: x.lower())\n",
    "\n",
    "\n",
    "#Removes the utf characters\n",
    "\n",
    "def utfremove(x):   #Need the \\ to escape the \"\n",
    "   return re.sub(r\"u'|u\\\"\", \"\", x)\n",
    "\n",
    "df[column_names] = df[column_names].applymap(utfremove)\n",
    "\n",
    "#Removes new line characters\n",
    "def nlremove(x):   #Need the \\ to escape the \"\n",
    "   return re.sub(r\"\\\\n\", \"\", x)\n",
    "\n",
    "\n",
    "#Removes hyperlinks\n",
    "\n",
    "def htmlremove(x):\n",
    "  return re.sub(r\"http\\S+\", \"\", x)\n",
    "\n",
    "df[column_names] = df[column_names].applymap(htmlremove)\n",
    "\n",
    "\n",
    "#Removes punctuation\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "df[column_names] = df[column_names].applymap(strip_punctuation)\n",
    "\n",
    "\n",
    "\n",
    "#Removes numbers\n",
    "\n",
    "def numremove(x):\n",
    "    return  re.sub(\"\\d+\", \"\", x)\n",
    "\n",
    "df[column_names] = df[column_names].applymap(numremove)\n",
    "\n",
    "\n",
    "\n",
    "#Removes stopwords\n",
    "def stopremove(x):\n",
    " from nltk.corpus import stopwords\n",
    " stop = stopwords.words('english')\n",
    " querywords = x.split()\n",
    "  \n",
    "\n",
    " stopwords= list(stop_words.ENGLISH_STOP_WORDS)\n",
    " resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    " result = ' '.join(resultwords) \n",
    " return(result)\n",
    "\n",
    "\n",
    "#Removes the stop words\n",
    "df[column_names] = df[column_names].applymap(stopremove)\n",
    "\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document term matrix and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tfidf stage\n",
    "\n",
    "#Maximum number of features\n",
    "n_features=200\n",
    "\n",
    "x=df['abstract']\n",
    "\n",
    "\n",
    "# TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "#max_df gives the highest proportion of documents that words are allowed to appear in\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, max_features=n_features, stop_words='english',ngram_range=(1,2))\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(x)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "#Converts the tfidf to a data frame which can be viewed\n",
    "tfidfdata=pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Use tf (raw term count) features\n",
    "tf_vectorizer = CountVectorizer(max_df=0.8, min_df=5,  max_features=n_features,stop_words='english', ngram_range=(1,2))\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(x)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import print_function\n",
    "\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]]) #argsort() returns the indices that sort an array\n",
    "        print(message)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \" \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "\n",
    "#Notes this needs python 3 to work\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "#Fits the model to the term inverse document frequency matrix\n",
    "lda.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names() #Gets the names of the words the tern frequency is defined over\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
